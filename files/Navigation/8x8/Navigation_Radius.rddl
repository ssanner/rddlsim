//////////////////////////////////////
//Buser.rddl
//
//Author:Buser
//////////////////////////////////////

domain Navigation_Problem{

	requirements = { 
		reward-deterministic 
	};

	types {
		dim: object;
	};
	
	pvariables {
	
		// Constant
		MINMAZEBOUND(dim): { non-fluent, real, default = 0.0 };
		MAXMAZEBOUND(dim): { non-fluent, real, default = 8.0 };
		MINACTIONBOUND(dim): { non-fluent, real, default = -1.0 };
		MAXACTIONBOUND(dim): { non-fluent, real, default = 1.0 };
		GOAL(dim): { non-fluent, real, default = 7.0 };
		PENALTY: {non-fluent, real, default = 1000000.0 };
		CENTER(dim): {non-fluent, real, default = 4.0};
		
		// Interm		
		distance: {interm-fluent,real,level=1 };
		scalefactor: {interm-fluent,real,level=2 };
		proposedLoc(dim):{interm-fluent, real, level=3};
 
		//State
		location(dim): {state-fluent, real, default = 0.0 };
				
		//Action
		move(dim): { action-fluent, real, default = 0.0 };
	};
	
	cpfs {

		distance = sqrt[sum_{?l:dim}[pow[(location(?l)-CENTER(?l)),2]]];
		scalefactor = 2.0/(1.0+exp[-2*distance])-0.99;
		proposedLoc(?l) = location(?l) + move(?l)*scalefactor;
		location'(?l)= if(proposedLoc(?l)<=MAXMAZEBOUND(?l) ^ proposedLoc(?l)>=MINMAZEBOUND(?l)) then proposedLoc(?l) 
					else (if(proposedLoc(?l)>MAXMAZEBOUND(?l)) then MAXMAZEBOUND(?l) else MINMAZEBOUND(?l));


	};
	
	reward = - sum_{?l: dim}[abs[GOAL(?l) - location(?l)]];
								
	state-action-constraints {
		forall_{?l:dim} move(?l)<=MAXACTIONBOUND(?l);
		forall_{?l:dim} move(?l)>=MINACTIONBOUND(?l);
		forall_{?l:dim} location(?l)<=MAXMAZEBOUND(?l);
		forall_{?l:dim} location(?l)>=MINMAZEBOUND(?l);
	};

}

non-fluents Navigation_non {
	domain = Navigation_Problem;
	objects{
		dim: {x,y};
	};
	non-fluents {
		MINMAZEBOUND(x) = 0.0;
	};
}

instance is1{
	domain = Navigation_Problem;
	non-fluents = Navigation_non;
	init-state{
		location(x) = 0.0;
		location(y) = 0.0;
	};
	max-nondef-actions = 2;
	horizon = 40;
	discount = 1.0;
}

